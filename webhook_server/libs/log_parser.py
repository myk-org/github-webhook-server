"""Log parsing and filtering functionality for webhook server logs."""

import asyncio
import datetime
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Any, AsyncGenerator


@dataclass
class LogEntry:
    """Represents a parsed log entry with structured data."""

    timestamp: datetime.datetime
    level: str
    logger_name: str
    message: str
    hook_id: str | None = None
    event_type: str | None = None
    repository: str | None = None
    pr_number: int | None = None
    github_user: str | None = None

    def to_dict(self) -> dict[str, Any]:
        """Convert LogEntry to dictionary for JSON serialization."""
        return {
            "timestamp": self.timestamp.isoformat(),
            "level": self.level,
            "logger_name": self.logger_name,
            "message": self.message,
            "hook_id": self.hook_id,
            "event_type": self.event_type,
            "repository": self.repository,
            "pr_number": self.pr_number,
            "github_user": self.github_user,
        }


class LogParser:
    """Parser for webhook server log files.

    Parses logs generated by GithubWebhook.prepare_log_prefix() function which creates
    structured log prefixes for webhook processing.

    Production logs location: /mnt/nfs/mediaserver/docker-compose/services/github-webhook-server/data-myakove/webhook_server/logs
    """

    # Regex pattern for parsing production logs from prepare_log_prefix() in github_api.py
    # Format from prepare_log_prefix():
    #   With PR: "{colored_repo} [{event}][{delivery_id}][{user}][PR {number}]: {message}"
    #   Without PR: "{colored_repo} [{event}][{delivery_id}][{user}]: {message}"
    # Full log format: "timestamp logger level colored_repo [event][delivery_id][user][PR number]: message"
    # Example: "2025-07-31T10:30:00.123000 GithubWebhook INFO repo-name [pull_request][abc123][user][PR 123]: Processing webhook"
    LOG_PATTERN = re.compile(
        r"^(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+) (\w+) (?:\x1b\[[\d;]*m)?(\w+)(?:\x1b\[[\d;]*m)? (.+)$"
    )

    # Pattern to extract GitHub context from prepare_log_prefix format
    # Matches: colored_repo [event][delivery_id][user][PR number]: message
    GITHUB_CONTEXT_PATTERN = re.compile(
        r"(?:\x1b\[[0-9;]*m)?([^\x1b\[\s]+)(?:\x1b\[[0-9;]*m)? \[([^\]]+)\]\[([^\]]+)\]\[([^\]]+)\](?:\[PR (\d+)\])?: (.+)"
    )

    ANSI_ESCAPE_PATTERN = re.compile(r"\x1b\[[0-9;]*m")

    def is_workflow_step(self, entry: LogEntry) -> bool:
        """
        Check if a log entry is a workflow step (logger.step call).

        Args:
            entry: LogEntry to check

        Returns:
            True if this is a workflow step entry
        """
        return entry.level.upper() == "STEP"

    def extract_workflow_steps(self, entries: list[LogEntry], hook_id: str) -> list[LogEntry]:
        """
        Extract workflow step entries for a specific hook ID.

        Args:
            entries: List of log entries to filter
            hook_id: Hook ID to filter by

        Returns:
            List of workflow step entries for the specified hook ID
        """
        return [entry for entry in entries if entry.hook_id == hook_id and self.is_workflow_step(entry)]

    def parse_log_entry(self, log_line: str) -> LogEntry | None:
        """
        Parse a single log line into a LogEntry object.

        Args:
            log_line: Raw log line string

        Returns:
            LogEntry object if parsing successful, None otherwise
        """
        if not log_line.strip():
            return None

        # Parse production log format
        match = self.LOG_PATTERN.match(log_line.strip())
        if not match:
            return None

        timestamp_str, logger_name, level, message = match.groups()

        # Parse ISO timestamp format: "2025-07-31T10:30:00.123000"
        try:
            timestamp = datetime.datetime.fromisoformat(timestamp_str)
        except ValueError:
            return None

        # Extract GitHub webhook context from prepare_log_prefix format
        repository, event_type, hook_id, github_user, pr_number, cleaned_message = self._extract_github_context(message)

        return LogEntry(
            timestamp=timestamp,
            level=level,
            logger_name=logger_name,
            message=cleaned_message,
            hook_id=hook_id,
            event_type=event_type,
            repository=repository,
            pr_number=pr_number,
            github_user=github_user,
        )

    def _extract_github_context(
        self, message: str
    ) -> tuple[str | None, str | None, str | None, str | None, int | None, str]:
        """Extract GitHub context from prepare_log_prefix format.

        Returns:
            Tuple of (repository, event_type, hook_id, github_user, pr_number, cleaned_message)
        """
        match = self.GITHUB_CONTEXT_PATTERN.search(message)
        if match:
            repository = match.group(1)
            event_type = match.group(2)
            hook_id = match.group(3)
            github_user = match.group(4)
            pr_number_str = match.group(5)  # Optional PR number
            cleaned_message = match.group(6)

            # Parse PR number if present
            pr_number = None
            if pr_number_str:
                try:
                    pr_number = int(pr_number_str)
                except ValueError:
                    pass

            # Clean ANSI codes from message
            cleaned_message = self.ANSI_ESCAPE_PATTERN.sub("", cleaned_message)

            return repository, event_type, hook_id, github_user, pr_number, cleaned_message

        # No GitHub context found, return original message cleaned of ANSI codes
        cleaned_message = self.ANSI_ESCAPE_PATTERN.sub("", message)
        return None, None, None, None, None, cleaned_message

    def parse_log_file(self, file_path: Path) -> list[LogEntry]:
        """
        Parse an entire log file and return list of LogEntry objects.

        Args:
            file_path: Path to the log file

        Returns:
            List of successfully parsed LogEntry objects
        """
        entries: list[LogEntry] = []
        total_lines = 0
        failed_lines = 0

        try:
            with open(file_path, "r", encoding="utf-8") as f:
                for line_num, line in enumerate(f, 1):
                    total_lines += 1
                    entry = self.parse_log_entry(line)
                    if entry:
                        entries.append(entry)
                    else:
                        failed_lines += 1

        except (OSError, UnicodeDecodeError):
            pass

        return entries

    async def tail_log_file(self, file_path: Path, follow: bool = True) -> AsyncGenerator[LogEntry, None]:
        """
        Tail a log file and yield new LogEntry objects as they are added.

        Args:
            file_path: Path to the log file to monitor
            follow: Whether to continue monitoring for new entries

        Yields:
            LogEntry objects for new log lines
        """
        # Start from the end of the file
        if not file_path.exists():
            return

        with open(file_path, "r", encoding="utf-8") as f:
            # Move to end of file
            f.seek(0, 2)

            while True:
                line = f.readline()
                if line:
                    entry = self.parse_log_entry(line)
                    if entry:
                        yield entry
                elif follow:
                    # No new data, wait a bit before checking again
                    await asyncio.sleep(0.1)
                else:
                    # Not following, exit when no more data
                    break

    async def monitor_log_directory(self, log_dir: Path, pattern: str = "*.log") -> AsyncGenerator[LogEntry, None]:
        """
        Monitor a directory for log files and yield new entries from all files.

        Args:
            log_dir: Directory path containing log files
            pattern: Glob pattern for log files (default: "*.log")

        Yields:
            LogEntry objects from all monitored log files
        """
        if not log_dir.exists() or not log_dir.is_dir():
            return

        # Find all existing log files including rotated ones
        log_files: list[Path] = []
        log_files.extend(log_dir.glob("*.log"))
        # Only monitor current log file, not rotated ones for real-time
        current_log_files = [
            f for f in log_files if not any(f.name.endswith(ext) for ext in [".1", ".2", ".3", ".4", ".5"])
        ]

        if not current_log_files:
            return

        # Monitor the most recent current log file (not rotated)
        # Sort by modification time to get the most recent file
        current_log_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        most_recent_file = current_log_files[0]

        async for entry in self.tail_log_file(most_recent_file, follow=True):
            yield entry


class LogFilter:
    """Filter log entries based on various criteria."""

    def filter_entries(
        self,
        entries: list[LogEntry],
        hook_id: str | None = None,
        pr_number: int | None = None,
        repository: str | None = None,
        event_type: str | None = None,
        github_user: str | None = None,
        level: str | None = None,
        start_time: datetime.datetime | None = None,
        end_time: datetime.datetime | None = None,
        search_text: str | None = None,
        limit: int | None = None,
        offset: int | None = None,
    ) -> list[LogEntry]:
        """
        Filter log entries based on provided criteria.

        Args:
            entries: List of LogEntry objects to filter
            hook_id: Filter by exact hook ID match
            pr_number: Filter by exact PR number match
            repository: Filter by exact repository match
            event_type: Filter by event type (supports partial matching)
            github_user: Filter by exact GitHub user match
            level: Filter by exact log level match
            start_time: Filter entries after this timestamp
            end_time: Filter entries before this timestamp
            search_text: Filter by text search in message (case-insensitive)
            limit: Maximum number of entries to return
            offset: Number of entries to skip (for pagination)

        Returns:
            Filtered list of LogEntry objects
        """
        filtered = entries[:]

        # Apply filters
        if hook_id is not None:
            filtered = [e for e in filtered if e.hook_id == hook_id]

        if pr_number is not None:
            filtered = [e for e in filtered if e.pr_number == pr_number]

        if repository is not None:
            filtered = [e for e in filtered if e.repository == repository]

        if event_type is not None:
            filtered = [e for e in filtered if e.event_type and event_type in e.event_type]

        if github_user is not None:
            filtered = [e for e in filtered if e.github_user == github_user]

        if level is not None:
            filtered = [e for e in filtered if e.level == level]

        if start_time is not None:
            filtered = [e for e in filtered if e.timestamp >= start_time]

        if end_time is not None:
            filtered = [e for e in filtered if e.timestamp <= end_time]

        if search_text is not None:
            search_lower = search_text.lower()
            filtered = [e for e in filtered if search_lower in e.message.lower()]

        # Apply pagination
        if offset is not None:
            filtered = filtered[offset:]

        if limit is not None:
            filtered = filtered[:limit]

        return filtered

    def get_unique_values(self, entries: list[LogEntry], field: str) -> list[str]:
        """
        Get unique values for a specific field across all entries.

        Args:
            entries: List of LogEntry objects
            field: Field name to get unique values for

        Returns:
            List of unique non-None values for the specified field
        """
        values = set()
        for entry in entries:
            value = getattr(entry, field, None)
            if value is not None:
                values.add(str(value))
        return sorted(list(values))

    def get_entry_count_by_field(self, entries: list[LogEntry], field: str) -> dict[str, int]:
        """
        Get count of entries grouped by a specific field.

        Args:
            entries: List of LogEntry objects
            field: Field name to group by

        Returns:
            Dictionary mapping field values to entry counts
        """
        counts: dict[str, int] = {}
        for entry in entries:
            value = getattr(entry, field, None)
            if value is not None:
                key = str(value)
                counts[key] = counts.get(key, 0) + 1
        return counts
